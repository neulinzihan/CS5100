{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngit add .\\ngit commit -m \"update recorded\" \\ngit push\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "git add .\n",
    "git commit -m \"update recorded\" \n",
    "git push\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Constants representing the state of each cell\n",
    "UNEXPLORED = -2\n",
    "OBSTACLE = -1\n",
    "SAFE = 0\n",
    "\n",
    "# Initialize the global grid status as unexplored\n",
    "global_grid = np.full((10, 10), UNEXPLORED)\n",
    "\n",
    "def drone_scan(drone_pos, scan_range, actual_env):\n",
    "    \"\"\"\n",
    "    Perform a local scan around a drone position.\n",
    "\n",
    "    Args:\n",
    "        drone_pos (tuple): Drone coordinates (x, y).\n",
    "        scan_range (int): The range of the drone's field of view (e.g., 3 or 5).\n",
    "        actual_env (np.array): The actual environment (10x10 matrix).\n",
    "\n",
    "    Returns:\n",
    "        tuple: local scanned information and top-left position of the scan.\n",
    "    \"\"\"\n",
    "    half_range = scan_range // 2\n",
    "    local_info = np.full((scan_range, scan_range), UNEXPLORED)\n",
    "\n",
    "    for i in range(scan_range):\n",
    "        for j in range(scan_range):\n",
    "            global_x = drone_pos[0] - half_range + i\n",
    "            global_y = drone_pos[1] - half_range + j\n",
    "\n",
    "            # Check boundaries\n",
    "            if 0 <= global_x < 10 and 0 <= global_y < 10:\n",
    "                local_info[i, j] = actual_env[global_x, global_y]\n",
    "                #print(f\"‚ö†Ô∏è Warning: Drone scan out of bounds at ({global_x}, {global_y})\")\n",
    "\n",
    "    return local_info, (drone_pos[0] - half_range, drone_pos[1] - half_range)\n",
    "\n",
    "def stitch_information(global_grid, local_info, top_left):\n",
    "    \"\"\"\n",
    "    Merge local drone scans into the global coverage grid.\n",
    "\n",
    "    Args:\n",
    "        global_grid (np.array): Current global grid state.\n",
    "        local_info (np.array): Local scan result from a drone.\n",
    "        top_left (tuple): Top-left coordinate of the local scan in the global grid.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Updated global grid.\n",
    "    \"\"\"\n",
    "    # find top left corner of local scan for better aligning and stitching\n",
    "    x_offset, y_offset = top_left\n",
    "\n",
    "    for i in range(local_info.shape[0]):\n",
    "        for j in range(local_info.shape[1]):\n",
    "            x, y = x_offset + i, y_offset + j\n",
    "            if 0 <= x < 10 and 0 <= y < 10:\n",
    "                # Update global grid only if it's unexplored\n",
    "                if global_grid[x, y] == UNEXPLORED:\n",
    "                    global_grid[x, y] = local_info[i, j]\n",
    "                # If conflicting info, prioritize safe information\n",
    "                elif global_grid[x, y] != local_info[i, j]:\n",
    "                    if local_info[i, j] == SAFE:\n",
    "                        global_grid[x, y] = SAFE\n",
    "    return global_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example actual environment (randomly generated obstacles and safe zones when searching)\n",
    "actual_env = np.random.choice([OBSTACLE, SAFE], size=(10, 10), p=[0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training to put drones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env, spaces\n",
    "import numpy as np\n",
    "\n",
    "class DronePlacementEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.max_steps = 50\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 10 x 10 grid √ó 2 drone types = 200 possible actions\n",
    "        self.action_space = spaces.Discrete(200)\n",
    "        self.observation_space = spaces.Box(low=UNEXPLORED, high=SAFE,shape=(self.grid_size, self.grid_size), dtype=np.int32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.actual_env = np.random.choice([OBSTACLE, SAFE], size=(self.grid_size, self.grid_size), p=[0.2, 0.8]).astype(np.int32)\n",
    "\n",
    "        self.global_grid = np.full((self.grid_size, self.grid_size), UNEXPLORED, dtype=np.int32)\n",
    "        self.current_step = 0\n",
    "\n",
    "        # ‚úÖ ÂºïÂØºÊé¢Á¥¢ÔºöÈ¢ÑÂÖàÊääËæπÁºòÈöèÊú∫Âá†‰∏™ÁÇπËÆæ‰∏∫ SAFE\n",
    "        edge_indices = list(range(self.grid_size))\n",
    "        edge_cells = []\n",
    "\n",
    "        # top and bottom rows\n",
    "        for j in edge_indices:\n",
    "            edge_cells.append((0, j))         # top row\n",
    "            edge_cells.append((self.grid_size - 1, j))  # bottom row\n",
    "\n",
    "        # left and right columnsÔºàÂéªÊéâ corners ÈÅøÂÖçÈáçÂ§çÔºâ\n",
    "        for i in edge_indices[1:-1]:\n",
    "            edge_cells.append((i, 0))         # left col\n",
    "            edge_cells.append((i, self.grid_size - 1))  # right col\n",
    "\n",
    "        # ÈöèÊú∫ÈÄâ 10 ‰∏™ËæπÁºòÊ†ºÂ≠êÂ°´ÂÖ•Â∑≤Áü• SAFE\n",
    "        np.random.shuffle(edge_cells)\n",
    "        for (x, y) in edge_cells[:10]:\n",
    "            self.global_grid[x, y] = self.actual_env[x, y]\n",
    "\n",
    "        # ÊòéÁ°ÆÊö¥Èú≤ÊúÄÂ∫ïÈÉ®Âá†Ë°åÂú∞Âõæ‰ø°ÊÅØÔºàÂº∫Ë°å agent ÁúãÂæóËßÅÔºâ\n",
    "        for row in [7, 8, 9]:\n",
    "            self.global_grid[row, :] = self.actual_env[row, :]\n",
    "\n",
    "        return self.global_grid.copy(), {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "\n",
    "        x = action // 20\n",
    "        y = (action % 20) // 2\n",
    "        drone_type = action % 2\n",
    "        scan_range = 3 if drone_type == 0 else 5\n",
    "\n",
    "        local_info, top_left = drone_scan((x, y), scan_range, self.actual_env)\n",
    "        prev_unexplored = np.sum(self.global_grid == UNEXPLORED)\n",
    "        self.global_grid = stitch_information(self.global_grid, local_info, top_left)\n",
    "        new_unexplored = np.sum(self.global_grid == UNEXPLORED)\n",
    "\n",
    "        # reward = new coverage - drone penalty - large drone penalty\n",
    "        reward = float(prev_unexplored - new_unexplored - 0.2 - 0.2 * drone_type)\n",
    "\n",
    "        # ËæπÁºòÊé¢Á¥¢Â•ñÂä±ÔºöÂ¶ÇÊûú drone Èù†ËøëËæπÁºòÔºåÂ∞±ÁªôÈºìÂä±\n",
    "        if x <= 1 or x >= 8 or y <= 1 or y >= 8:\n",
    "            reward += 0.3\n",
    "\n",
    "        terminated = bool(new_unexplored == 0)\n",
    "        truncated = bool(self.current_step >= self.max_steps)\n",
    "\n",
    "        # Â•ñÂä±ÂÆåÊàêÂú∞ÂõæÊé¢Á¥¢\n",
    "        if terminated:\n",
    "            reward += 10  # ÊàêÂäüË¶ÜÁõñÂÖ®ÂõæÂ•ñÂä±\n",
    "        elif truncated and np.sum(self.global_grid == UNEXPLORED) > 0:\n",
    "            reward -= 5   # ËææÂà∞ÊúÄÂ§ßÊ≠•Êï∞Âç¥Ê≤°Êâ´ÂÆåÔºåÊÉ©ÁΩö\n",
    "\n",
    "\n",
    "        for row in [7, 8, 9]:\n",
    "            if np.all(self.global_grid[row, :] != UNEXPLORED):\n",
    "                reward += 3  # ÊØèË¶ÜÁõñ‰∏ÄÊï¥Ë°åÂ•ñÂä± 3 ÂàÜ\n",
    "\n",
    "\n",
    "\n",
    "        return self.global_grid.copy(), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(self.global_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a small grid 6*6, then the transfer to 10*10 grid\n",
    "\n",
    "class SmallDroneEnv(DronePlacementEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 6\n",
    "        self.max_steps = 8  # ÈÄÇÂêàÂ∞èÂú∞ÂõæÁöÑ max steps\n",
    "        self.action_space = spaces.Discrete(self.grid_size * self.grid_size * 2)\n",
    "        self.observation_space = spaces.Box(low=UNEXPLORED, high=SAFE, shape=(6, 6), dtype=np.int32)\n",
    "\n",
    "def train_on_small_map():\n",
    "    env = SmallDroneEnv()\n",
    "    check_env(env)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=50000)\n",
    "    model.save(\"ppo_small_model\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "def train_with_ppo():\n",
    "    env = DronePlacementEnv()\n",
    "    check_env(env)\n",
    "    model = PPO.load(\"ppo_small_model\", env=env, verbose=1)\n",
    "    model.learn(total_timesteps=1e5)\n",
    "    model.save(\"ppo_large_model\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trained_model():\n",
    "    model = PPO.load(\"ppo_drone_model\")\n",
    "    env = DronePlacementEnv()\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    print(\"=== DRONE DEPLOYMENT SEQUENCE ===\")\n",
    "    for step in range(env.max_steps):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        x = action // 20\n",
    "        y = (action % 20) // 2\n",
    "        drone_type = \"3x3\" if action % 2 == 0 else \"5x5\"\n",
    "        print(f\"Step {step}: Placed {drone_type} drone at ({x}, {y}), reward: {reward:.2f}\")\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        if terminated:\n",
    "            print(\"üéâ Mission Complete: All cells explored!\")\n",
    "            break\n",
    "        if truncated:\n",
    "            print(\"‚ö†Ô∏è Max steps reached.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Placed 3x3 drone at (np.int64(0), np.int64(6)), reward = 9.10\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -2 -2  0  0  0 -2 -1]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 1: Placed 3x3 drone at (np.int64(3), np.int64(3)), reward = 8.80\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -2 -2  0  0  0 -2 -1]\n",
      " [-2 -2 -1 -1 -1 -2 -2 -2 -2 -2]\n",
      " [-2 -2  0  0  0 -2 -2 -2 -2 -2]\n",
      " [-1 -2  0  0  0 -2 -2 -2 -2 -2]\n",
      " [ 0 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 2: Placed 5x5 drone at (np.int64(3), np.int64(7)), reward = 8.60\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -2 -2  0  0  0 -1 -1]\n",
      " [-2 -2 -1 -1 -1  0  0 -1  0  0]\n",
      " [-2 -2  0  0  0  0  0  0 -1  0]\n",
      " [-1 -2  0  0  0  0 -1  0 -1  0]\n",
      " [ 0 -2 -2 -2 -2  0  0 -1  0  0]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 3: Placed 3x3 drone at (np.int64(2), np.int64(6)), reward = 8.80\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -2 -2  0  0  0 -1 -1]\n",
      " [-2 -2 -1 -1 -1  0  0 -1  0  0]\n",
      " [-2 -2  0  0  0  0  0  0 -1  0]\n",
      " [-1 -2  0  0  0  0 -1  0 -1  0]\n",
      " [ 0 -2 -2 -2 -2  0  0 -1  0  0]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 4: Placed 3x3 drone at (np.int64(3), np.int64(0)), reward = 9.10\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -2 -2  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0 -2 -2 -2 -2  0  0 -1  0  0]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 5: Placed 3x3 drone at (np.int64(2), np.int64(4)), reward = 8.80\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0 -2 -2 -2 -2  0  0 -1  0  0]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 6: Placed 3x3 drone at (np.int64(2), np.int64(5)), reward = 8.80\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0 -2 -2 -2 -2  0  0 -1  0  0]\n",
      " [-1 -2 -2 -2 -2 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 7: Placed 5x5 drone at (np.int64(4), np.int64(2)), reward = 8.60\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 8: Placed 3x3 drone at (np.int64(3), np.int64(6)), reward = 8.80\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 9: Placed 3x3 drone at (np.int64(4), np.int64(8)), reward = 9.10\n",
      "[[-2 -2 -2 -2 -2  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 10: Placed 5x5 drone at (np.int64(1), np.int64(6)), reward = 8.90\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 11: Placed 3x3 drone at (np.int64(3), np.int64(0)), reward = 9.10\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 12: Placed 3x3 drone at (np.int64(3), np.int64(2)), reward = 8.80\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 13: Placed 5x5 drone at (np.int64(0), np.int64(6)), reward = 8.90\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1 -2]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 14: Placed 5x5 drone at (np.int64(2), np.int64(7)), reward = 8.60\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1  0]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 15: Placed 3x3 drone at (np.int64(3), np.int64(3)), reward = 8.80\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1  0]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0 -2 -2 -2 -2  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 16: Placed 5x5 drone at (np.int64(4), np.int64(6)), reward = 8.60\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1  0]\n",
      " [-2 -2 -2 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 17: Placed 5x5 drone at (np.int64(3), np.int64(4)), reward = 8.60\n",
      "[[-2 -2 -2 -2  0  0 -1  0 -1  0]\n",
      " [-2 -2 -1 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "Step 18: Placed 5x5 drone at (np.int64(1), np.int64(1)), reward = 18.90\n",
      "[[ 0  0  0  0  0  0 -1  0 -1  0]\n",
      " [-1  0 -1 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n",
      "üéâ ÈÉ®ÁΩ≤ÂÆåÊàêÔºöÊâÄÊúâÊ†ºÂ≠êÂ∑≤Ë¶ÜÁõñÔºÅ\n",
      "Final Global Grid after Stitching:\n",
      "[[ 0  0  0  0  0  0 -1  0 -1  0]\n",
      " [-1  0 -1 -1  0  0  0  0 -1 -1]\n",
      " [ 0  0 -1 -1 -1  0  0 -1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0 -1  0]\n",
      " [ 0  0  0 -1  0  0  0 -1  0  0]\n",
      " [-1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0  0  0  0]\n",
      " [-1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0  0 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"ppo_drone_model\")\n",
    "\n",
    "env = DronePlacementEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for step in range(env.max_steps):  # ÊúÄÂ§öÊîæ max_steps Êû∂ drone\n",
    "    action, _ = model.predict(obs)\n",
    "\n",
    "    # Ëß£Á†ÅÂä®‰Ωú\n",
    "    x = action // 20\n",
    "    y = (action % 20) // 2\n",
    "    drone_type = action % 2\n",
    "    scan_range = 3 if drone_type == 0 else 5\n",
    "    pos = (x, y)\n",
    "\n",
    "    # Ê®°ÊãüÊâ´Êèè + ÊãºÊé•\n",
    "    local_info, top_left = drone_scan(pos, scan_range, env.actual_env)\n",
    "    global_grid = stitch_information(env.global_grid, local_info, top_left)\n",
    "\n",
    "    # ÁúüÊ≠£ÊâßË°å‰∏ÄÊ≠•\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    print(f\"Step {step}: Placed {scan_range}x{scan_range} drone at {pos}, reward = {reward:.2f}\")\n",
    "    env.render()\n",
    "\n",
    "    if terminated:\n",
    "        print(\"üéâ ÈÉ®ÁΩ≤ÂÆåÊàêÔºöÊâÄÊúâÊ†ºÂ≠êÂ∑≤Ë¶ÜÁõñÔºÅ\")\n",
    "        break\n",
    "    if truncated:\n",
    "        print(\"‚ö†Ô∏è ËææÂà∞ÊúÄÂ§ßÊ≠•Êï∞\")\n",
    "        break\n",
    "\n",
    "print(\"Final Global Grid after Stitching:\")\n",
    "print(env.global_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchsafe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
