{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngit add .\\ngit commit -m \"update recorded\" \\ngit push\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "git add .\n",
    "git commit -m \"update recorded\" \n",
    "git push\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import torch\n",
    "\n",
    "\n",
    "from stable_baselines3.common.torch_layers import MlpExtractor      # for paramete of model\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128, 64], vf=[128, 64])],  # policy and value net\n",
    ")\n",
    "\n",
    "\n",
    "# Constants representing the state of each cell\n",
    "UNEXPLORED = -2\n",
    "OBSTACLE = -1\n",
    "SAFE = 0\n",
    "\n",
    "# Initialize the global grid status as unexplored\n",
    "global_grid = np.full((10, 10), UNEXPLORED)\n",
    "\n",
    "def drone_scan(drone_pos, scan_range, actual_env):\n",
    "    \"\"\"\n",
    "    Perform a local scan around a drone position.\n",
    "    \"\"\"\n",
    "    half_range = scan_range // 2\n",
    "    local_info = np.full((scan_range, scan_range), UNEXPLORED)\n",
    "\n",
    "    grid_h, grid_w = actual_env.shape  # ‚úÖ Âä®ÊÄÅËé∑ÂèñÂú∞ÂõæÂ§ßÂ∞èÔºÅ\n",
    "\n",
    "    for i in range(scan_range):\n",
    "        for j in range(scan_range):\n",
    "            global_x = drone_pos[0] - half_range + i\n",
    "            global_y = drone_pos[1] - half_range + j\n",
    "\n",
    "            # ‚úÖ Áî®ÂÆûÈôÖÂú∞ÂõæÁöÑÂ§ßÂ∞èÂà§Êñ≠ËæπÁïå\n",
    "            if 0 <= global_x < grid_h and 0 <= global_y < grid_w:\n",
    "                local_info[i, j] = actual_env[global_x, global_y]\n",
    "\n",
    "    return local_info, (drone_pos[0] - half_range, drone_pos[1] - half_range)\n",
    "\n",
    "\n",
    "def stitch_information(global_grid, local_info, top_left):\n",
    "    \"\"\"\n",
    "    Merge local drone scans into the global coverage grid.\n",
    "\n",
    "    Args:\n",
    "        global_grid (np.array): Current global grid state.\n",
    "        local_info (np.array): Local scan result from a drone.\n",
    "        top_left (tuple): Top-left coordinate of the local scan in the global grid.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Updated global grid.\n",
    "    \"\"\"\n",
    "    x_offset, y_offset = top_left\n",
    "    grid_h, grid_w = global_grid.shape  # ‚úÖ Âä®ÊÄÅËé∑ÂèñÂú∞ÂõæÂ§ßÂ∞èÔºÅ\n",
    "\n",
    "    for i in range(local_info.shape[0]):\n",
    "        for j in range(local_info.shape[1]):\n",
    "            x, y = x_offset + i, y_offset + j\n",
    "            if 0 <= x < grid_h and 0 <= y < grid_w:\n",
    "                if global_grid[x, y] == UNEXPLORED:\n",
    "                    global_grid[x, y] = local_info[i, j]\n",
    "                elif global_grid[x, y] != local_info[i, j]:\n",
    "                    if local_info[i, j] == SAFE:\n",
    "                        global_grid[x, y] = SAFE\n",
    "    return global_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example actual environment (randomly generated obstacles and safe zones when searching)\n",
    "actual_env = np.random.choice([OBSTACLE, SAFE], size=(10, 10), p=[0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training to put drones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env, spaces\n",
    "import numpy as np\n",
    "\n",
    "class DronePlacementEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 10\n",
    "        self.max_steps = 50\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 10 x 10 grid √ó 2 drone types = 200 possible actions\n",
    "        self.action_space = spaces.Discrete(200)\n",
    "        self.observation_space = spaces.Box(low=UNEXPLORED, high=SAFE,shape=(self.grid_size, self.grid_size), dtype=np.int32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.actual_env = np.random.choice([OBSTACLE, SAFE], size=(self.grid_size, self.grid_size), p=[0.2, 0.8]).astype(np.int32)\n",
    "\n",
    "        self.global_grid = np.full((self.grid_size, self.grid_size), UNEXPLORED, dtype=np.int32)\n",
    "        self.current_step = 0\n",
    "\n",
    "        # ‚úÖ ÂºïÂØºÊé¢Á¥¢ÔºöÈ¢ÑÂÖàÊääËæπÁºòÈöèÊú∫Âá†‰∏™ÁÇπËÆæ‰∏∫ SAFE\n",
    "        edge_indices = list(range(self.grid_size))\n",
    "        edge_cells = []\n",
    "\n",
    "        # top and bottom rows\n",
    "        for j in edge_indices:\n",
    "            edge_cells.append((0, j))         # top row\n",
    "            edge_cells.append((self.grid_size - 1, j))  # bottom row\n",
    "\n",
    "        # left and right columnsÔºàÂéªÊéâ corners ÈÅøÂÖçÈáçÂ§çÔºâ\n",
    "        for i in edge_indices[1:-1]:\n",
    "            edge_cells.append((i, 0))         # left col\n",
    "            edge_cells.append((i, self.grid_size - 1))  # right col\n",
    "\n",
    "        # ÈöèÊú∫ÈÄâ 10 ‰∏™ËæπÁºòÊ†ºÂ≠êÂ°´ÂÖ•Â∑≤Áü• SAFE\n",
    "        np.random.shuffle(edge_cells)\n",
    "        for (x, y) in edge_cells[:10]:\n",
    "            self.global_grid[x, y] = self.actual_env[x, y]\n",
    "\n",
    "        # ÊòéÁ°ÆÊö¥Èú≤Â∫ïÈÉ®ÊúÄÂ§ö 3 Ë°åÔºà‰∏çË∂ÖËøá grid ËæπÁïåÔºâ\n",
    "        for row in range(self.grid_size - 3, self.grid_size):\n",
    "            self.global_grid[row, :] = self.actual_env[row, :]\n",
    "\n",
    "\n",
    "        return self.global_grid.copy(), {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        action = int(min(action, self.grid_size * self.grid_size * 2 - 1))\n",
    "\n",
    "        action_per_row = self.grid_size * 2  # ÊØèË°å 2 ‰∏™Âä®‰ΩúÔºà3x3 + 5x5Ôºâ\n",
    "\n",
    "        x = action // action_per_row\n",
    "        y = (action % action_per_row) // 2\n",
    "        drone_type = action % 2\n",
    "\n",
    "        if not (0 <= x < self.grid_size and 0 <= y < self.grid_size):\n",
    "            raise ValueError(f\"Decoded position ({x}, {y}) is out of bounds!\")\n",
    "\n",
    "        scan_range = 3 if drone_type == 0 else 5\n",
    "\n",
    "        local_info, top_left = drone_scan((x, y), scan_range, self.actual_env)\n",
    "        prev_unexplored = np.sum(self.global_grid == UNEXPLORED)\n",
    "        self.global_grid = stitch_information(self.global_grid, local_info, top_left)\n",
    "        new_unexplored = np.sum(self.global_grid == UNEXPLORED)\n",
    "\n",
    "        # reward = new coverage - drone penalty - large drone penalty\n",
    "        reward = float(prev_unexplored - new_unexplored - 0.2 - 0.2 * drone_type)\n",
    "\n",
    "        # ËæπÁºòÊé¢Á¥¢Â•ñÂä±ÔºöÂ¶ÇÊûú drone Èù†ËøëËæπÁºòÔºåÂ∞±ÁªôÈºìÂä±\n",
    "        if x <= 1 or x >= 8 or y <= 1 or y >= 8:\n",
    "            reward += 0.3\n",
    "\n",
    "        terminated = bool(new_unexplored == 0)\n",
    "        truncated = bool(self.current_step >= self.max_steps)\n",
    "\n",
    "        # Â•ñÂä±ÂÆåÊàêÂú∞ÂõæÊé¢Á¥¢\n",
    "        if terminated:\n",
    "            reward += 10  # ÊàêÂäüË¶ÜÁõñÂÖ®ÂõæÂ•ñÂä±\n",
    "        elif truncated and np.sum(self.global_grid == UNEXPLORED) > 0:\n",
    "            reward -= 5   # ËææÂà∞ÊúÄÂ§ßÊ≠•Êï∞Âç¥Ê≤°Êâ´ÂÆåÔºåÊÉ©ÁΩö\n",
    "\n",
    "\n",
    "        return self.global_grid.copy(), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(self.global_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a small grid 6*6, then the transfer to 10*10 grid\n",
    "\n",
    "class SmallDroneEnv(DronePlacementEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grid_size = 6\n",
    "        self.max_steps = 8  # ÈÄÇÂêàÂ∞èÂú∞ÂõæÁöÑ max steps\n",
    "        self.action_space = spaces.Discrete(self.grid_size * self.grid_size * 2)\n",
    "        self.observation_space = spaces.Box(low=UNEXPLORED, high=SAFE, shape=(6, 6), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_small_map():\n",
    "    env = SmallDroneEnv()\n",
    "    check_env(env)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=2)\n",
    "    model.learn(total_timesteps=50000)\n",
    "    model.save(\"ppo_small_model\")\n",
    "\n",
    "    torch.save(model.policy.state_dict(), \"small_policy_weights.pt\")\n",
    "\n",
    "    return model\n",
    "'''\n",
    "def train_with_ppo():\n",
    "    env = DronePlacementEnv()\n",
    "    check_env(env)\n",
    "    #model = PPO(\"MlpPolicy\", env)  # only for first time\n",
    "    model = PPO.load(\"ppo_small_model\", env=env, verbose=1)\n",
    "    model.learn(total_timesteps=1e5)\n",
    "    model.save(\"ppo_large_model\")\n",
    "    return model\n",
    "'''\n",
    "def train_with_ppo():\n",
    "    env = DronePlacementEnv()\n",
    "    check_env(env)\n",
    "    \n",
    "    # ‰∏éÂ∞èÂú∞Âõæ‰øùÊåÅ‰∏ÄËá¥ÁöÑÁΩëÁªúÁªìÊûÑ\n",
    "    policy_kwargs = dict(net_arch=[dict(pi=[128, 64], vf=[128, 64])])\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs)\n",
    "\n",
    "    # ‚úÖ Âä†ËΩΩÂ∞èÊ®°ÂûãÂèÇÊï∞Âà∞Êñ∞Ê®°Âûã‰∏≠ÔºàÊ≥®ÊÑè strict=FalseÔºâ\n",
    "    import torch\n",
    "    model.policy.load_state_dict(torch.load(\"small_policy_weights.pt\"), strict=False)\n",
    "\n",
    "    model.learn(total_timesteps=10000)\n",
    "    model.save(\"ppo_large_model\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trained_model():\n",
    "    model = PPO.load(\"ppo_large_model\")\n",
    "    env = DronePlacementEnv()\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    print(\"=== DRONE DEPLOYMENT SEQUENCE ===\")\n",
    "    for step in range(env.max_steps):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        x = action // 20\n",
    "        y = (action % 20) // 2\n",
    "        drone_type = \"3x3\" if action % 2 == 0 else \"5x5\"\n",
    "        print(f\"Step {step}: Placed {drone_type} drone at ({x}, {y}), reward: {reward:.2f}\")\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        if terminated:\n",
    "            print(\"üéâ Mission Complete: All cells explored!\")\n",
    "            break\n",
    "        if truncated:\n",
    "            print(\"‚ö†Ô∏è Max steps reached.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:272: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.18     |\n",
      "|    ep_rew_mean     | 17.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 389      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.12        |\n",
      "|    ep_rew_mean          | 16.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 322         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014560357 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | -0.0144     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.6        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 72.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.46        |\n",
      "|    ep_rew_mean          | 18.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 306         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016791165 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.23        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 29.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.84        |\n",
      "|    ep_rew_mean          | 20.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 294         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017217075 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.2        |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.09        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.41        |\n",
      "|    ep_rew_mean          | 21.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 285         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017671328 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.14       |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.6         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 9.32        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.94        |\n",
      "|    ep_rew_mean          | 21.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 279         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018522175 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.06       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.57        |\n",
      "|    ep_rew_mean          | 22.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018505482 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.98       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 3.53        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.13        |\n",
      "|    ep_rew_mean          | 22.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015343312 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.23        |\n",
      "|    ep_rew_mean          | 22.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016568318 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.85       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.75        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.11        |\n",
      "|    ep_rew_mean          | 22.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015335407 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.76       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.735       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.92        |\n",
      "|    ep_rew_mean          | 22.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015898868 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.69       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.32        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.87        |\n",
      "|    ep_rew_mean          | 22.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015717134 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.61       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.363       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.68        |\n",
      "|    ep_rew_mean          | 23          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 269         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015733944 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.52       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.541       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.51        |\n",
      "|    ep_rew_mean          | 22.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015514306 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.44       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.306       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.8         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.58        |\n",
      "|    ep_rew_mean          | 23          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014489559 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.36       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.973       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.52        |\n",
      "|    ep_rew_mean          | 22.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014622081 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.25       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.256       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 0.709       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.55       |\n",
      "|    ep_rew_mean          | 22.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 253        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 137        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01426455 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.17      |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.181      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.622      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.39        |\n",
      "|    ep_rew_mean          | 22.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014436719 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.1        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.171       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    value_loss           | 0.774       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.44        |\n",
      "|    ep_rew_mean          | 22.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013622967 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.01       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.559       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.51        |\n",
      "|    ep_rew_mean          | 22.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013843607 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.322       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.628       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.42        |\n",
      "|    ep_rew_mean          | 23          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 166         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014017913 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.199       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.533       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.47        |\n",
      "|    ep_rew_mean          | 22.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013524193 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.196       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    value_loss           | 0.623       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5         |\n",
      "|    ep_rew_mean          | 23          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014197828 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.363       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    value_loss           | 0.519       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.28        |\n",
      "|    ep_rew_mean          | 22.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013932483 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.57        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.21        |\n",
      "|    ep_rew_mean          | 23          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013572735 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 0.431       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:272: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n",
      "d:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticPolicy:\n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([64, 36]) from checkpoint, the shape in current model is torch.Size([128, 100]).\n\tsize mismatch for mlp_extractor.policy_net.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_extractor.policy_net.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([64, 36]) from checkpoint, the shape in current model is torch.Size([128, 100]).\n\tsize mismatch for mlp_extractor.value_net.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_extractor.value_net.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for action_net.weight: copying a param with shape torch.Size([72, 64]) from checkpoint, the shape in current model is torch.Size([200, 64]).\n\tsize mismatch for action_net.bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([200]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     train_on_small_map()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain_with_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     run_trained_model()\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mtrain_with_ppo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# ‚úÖ Âä†ËΩΩÂ∞èÊ®°ÂûãÂèÇÊï∞Âà∞Êñ∞Ê®°Âûã‰∏≠ÔºàÊ≥®ÊÑè strict=FalseÔºâ\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmall_policy_weights.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_large_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticPolicy:\n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([64, 36]) from checkpoint, the shape in current model is torch.Size([128, 100]).\n\tsize mismatch for mlp_extractor.policy_net.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_extractor.policy_net.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([64, 36]) from checkpoint, the shape in current model is torch.Size([128, 100]).\n\tsize mismatch for mlp_extractor.value_net.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_extractor.value_net.2.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for action_net.weight: copying a param with shape torch.Size([72, 64]) from checkpoint, the shape in current model is torch.Size([200, 64]).\n\tsize mismatch for action_net.bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([200])."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_on_small_map()\n",
    "    train_with_ppo()\n",
    "    run_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_large_model\")\n",
    "\n",
    "env = DronePlacementEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for step in range(env.max_steps):  # ÊúÄÂ§öÊîæ max_steps Êû∂ drone\n",
    "    action, _ = model.predict(obs)\n",
    "\n",
    "    # Ëß£Á†ÅÂä®‰Ωú\n",
    "    x = action // 20\n",
    "    y = (action % 20) // 2\n",
    "    drone_type = action % 2\n",
    "    scan_range = 3 if drone_type == 0 else 5\n",
    "    pos = (x, y)\n",
    "\n",
    "    # Ê®°ÊãüÊâ´Êèè + ÊãºÊé•\n",
    "    local_info, top_left = drone_scan(pos, scan_range, env.actual_env)\n",
    "    global_grid = stitch_information(env.global_grid, local_info, top_left)\n",
    "\n",
    "    # ÁúüÊ≠£ÊâßË°å‰∏ÄÊ≠•\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    print(f\"Step {step}: Placed {scan_range}x{scan_range} drone at {pos}, reward = {reward:.2f}\")\n",
    "    env.render()\n",
    "\n",
    "    if terminated:\n",
    "        print(\"üéâ ÈÉ®ÁΩ≤ÂÆåÊàêÔºöÊâÄÊúâÊ†ºÂ≠êÂ∑≤Ë¶ÜÁõñÔºÅ\")\n",
    "        break\n",
    "    if truncated:\n",
    "        print(\"‚ö†Ô∏è ËææÂà∞ÊúÄÂ§ßÊ≠•Êï∞\")\n",
    "        break\n",
    "\n",
    "print(\"Final Global Grid after Stitching:\")\n",
    "print(env.global_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchsafe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
