{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605d9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gymnasium import Env, spaces\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c14cec",
   "metadata": {},
   "source": [
    "Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a668d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "UNEXPLORED = -2\n",
    "OBSTACLE = -1\n",
    "SAFE = 0\n",
    "\n",
    "# === Utility Functions ===\n",
    "def drone_scan(drone_pos, scan_range, actual_env):\n",
    "    half_range = scan_range // 2\n",
    "    local_info = np.full((scan_range, scan_range), UNEXPLORED)\n",
    "    grid_h, grid_w = actual_env.shape\n",
    "\n",
    "    for i in range(scan_range):\n",
    "        for j in range(scan_range):\n",
    "            global_x = drone_pos[0] - half_range + i\n",
    "            global_y = drone_pos[1] - half_range + j\n",
    "            if 0 <= global_x < grid_h and 0 <= global_y < grid_w:\n",
    "                local_info[i, j] = actual_env[global_x, global_y]\n",
    "\n",
    "    return local_info, (drone_pos[0] - half_range, drone_pos[1] - half_range)\n",
    "\n",
    "def stitch_information(global_grid, local_info, top_left):\n",
    "    x_offset, y_offset = top_left\n",
    "    grid_h, grid_w = global_grid.shape\n",
    "\n",
    "    for i in range(local_info.shape[0]):\n",
    "        for j in range(local_info.shape[1]):\n",
    "            x, y = x_offset + i, y_offset + j\n",
    "            if 0 <= x < grid_h and 0 <= y < grid_w:\n",
    "                if global_grid[x, y] == UNEXPLORED:\n",
    "                    global_grid[x, y] = local_info[i, j]\n",
    "                elif global_grid[x, y] != local_info[i, j]:\n",
    "                    if local_info[i, j] == SAFE:\n",
    "                        global_grid[x, y] = SAFE\n",
    "    return global_grid\n",
    "\n",
    "# === Custom Environment ===\n",
    "class DronePlacementEnv(Env):\n",
    "    def __init__(self, grid_size=15, max_steps=50):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.action_space = spaces.Discrete(grid_size * grid_size * 2)\n",
    "        self.observation_space = spaces.Box(low=UNEXPLORED, high=SAFE, shape=(grid_size, grid_size), dtype=np.int32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.actual_env = np.random.choice([OBSTACLE, SAFE], size=(self.grid_size, self.grid_size), p=[0.2, 0.8]).astype(np.int32)\n",
    "        self.global_grid = np.full((self.grid_size, self.grid_size), UNEXPLORED, dtype=np.int32)\n",
    "        self.current_step = 0\n",
    "        return self.global_grid.copy(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        action = int(min(action, self.grid_size * self.grid_size * 2 - 1))\n",
    "        action_per_row = self.grid_size * 2\n",
    "        x = action // action_per_row\n",
    "        y = (action % action_per_row) // 2\n",
    "        drone_type = action % 2\n",
    "\n",
    "        if not (0 <= x < self.grid_size and 0 <= y < self.grid_size):\n",
    "            raise ValueError(f\"Decoded position ({x}, {y}) is out of bounds!\")\n",
    "\n",
    "        scan_range = 3 if drone_type == 0 else 5\n",
    "        local_info, top_left = drone_scan((x, y), scan_range, self.actual_env)\n",
    "        prev_unexplored = np.sum(self.global_grid == UNEXPLORED)\n",
    "        self.global_grid = stitch_information(self.global_grid, local_info, top_left)\n",
    "        new_unexplored = np.sum(self.global_grid == UNEXPLORED)\n",
    "\n",
    "        reward = float(prev_unexplored - new_unexplored - 0.2 - 0.2 * drone_type)\n",
    "        if x <= 1 or x >= self.grid_size - 2 or y <= 1 or y >= self.grid_size - 2:\n",
    "            reward += 0.3\n",
    "\n",
    "        terminated = bool(new_unexplored == 0)\n",
    "        truncated = bool(self.current_step >= self.max_steps)\n",
    "        if terminated:\n",
    "            reward += 10\n",
    "        elif truncated:\n",
    "            reward -= 5\n",
    "\n",
    "        return self.global_grid.copy(), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(self.global_grid)\n",
    "\n",
    "# === Small Map Env ===\n",
    "class SmallDroneEnv(DronePlacementEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(grid_size=6, max_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90616948",
   "metadata": {},
   "source": [
    "Below block is training, ignore this when already have the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2899e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torchsafe\\lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.7     |\n",
      "|    ep_rew_mean     | 202      |\n",
      "| time/              |          |\n",
      "|    fps             | 402      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 49.9         |\n",
      "|    ep_rew_mean          | 203          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 330          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101379845 |\n",
      "|    clip_fraction        | 0.0765       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.11        |\n",
      "|    explained_variance   | -0.00922     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 760          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0625      |\n",
      "|    value_loss           | 1.65e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 50           |\n",
      "|    ep_rew_mean          | 202          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086192945 |\n",
      "|    clip_fraction        | 0.0601       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.1         |\n",
      "|    explained_variance   | 0.0793       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 992          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.057       |\n",
      "|    value_loss           | 1.71e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010107545 |\n",
      "|    clip_fraction        | 0.0852      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.09       |\n",
      "|    explained_variance   | 0.0897      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 692         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 1.65e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009245558 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.09       |\n",
      "|    explained_variance   | 0.0973      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 683         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 1.63e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010120669 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.09       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 885         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 1.51e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 201         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 294         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010184588 |\n",
      "|    clip_fraction        | 0.0851      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 721         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 1.45e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010323053 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 784         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 1.43e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011380379 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 468         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 1.35e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012411142 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.06       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 424         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 1.2e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 201         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012667756 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.06       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 494         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 1.05e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 201         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 288         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012408466 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 451         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 967         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 201         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013452071 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 200         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 857         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013905868 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.04       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 227         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 732         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.9       |\n",
      "|    ep_rew_mean          | 202        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 107        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01379515 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6.02      |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 358        |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0675    |\n",
      "|    value_loss           | 676        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 285         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012512216 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 122         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 550         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013347265 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 218         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 462         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.8        |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013570943 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.97       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 90.5        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 377         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.8        |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013420665 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.97       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 192         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 304         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.8        |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013203116 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.94       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 162         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 279         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012646433 |\n",
      "|    clip_fraction        | 0.0933      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.95       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 106         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 215         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013050079 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.94       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 137         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011168579 |\n",
      "|    clip_fraction        | 0.0837      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.92       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 71          |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013036933 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.89       |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.3        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012992097 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.3        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 92.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 188         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012067451 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.1         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 74.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012439575 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.83       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.3        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 58.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013982207 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.8        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 48.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014480126 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.97        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 39.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 279         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014256184 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.73       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 40.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 50           |\n",
      "|    ep_rew_mean          | 203          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 279          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 227          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142083075 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.72        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0547      |\n",
      "|    value_loss           | 29.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 234         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012994944 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.73       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 23          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013935394 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.72       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.78        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 22.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016341057 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 201         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013403823 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.7        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.56        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017524227 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.67       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 202         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016263358 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.68       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.66        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 11.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | 202        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 274        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 283        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01587367 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.65      |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.73       |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0633    |\n",
      "|    value_loss           | 10.2       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 49.8       |\n",
      "|    ep_rew_mean          | 203        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 274        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 290        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01581581 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.66      |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.55       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0581    |\n",
      "|    value_loss           | 12         |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 49.8         |\n",
      "|    ep_rew_mean          | 203          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 273          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 299          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135219805 |\n",
      "|    clip_fraction        | 0.115        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.7         |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.36         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0551      |\n",
      "|    value_loss           | 17           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017654445 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.66       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.89        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 8.76        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 203         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017162696 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.66       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 8.48        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 323         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018231533 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.67       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 8.22        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016060468 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.7        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.14        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 12.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017228412 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 8.42        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.9        |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015829597 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.68       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.81        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 10.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | 204        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 353        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01883879 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.79       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0727    |\n",
      "|    value_loss           | 8.14       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 50          |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015576158 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 50         |\n",
      "|    ep_rew_mean          | 204        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 368        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01893257 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.64      |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.56       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0767    |\n",
      "|    value_loss           | 7.77       |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# === Training Functions ===\n",
    "def save_small_model_encoder():\n",
    "    env = SmallDroneEnv()\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "    model.learn(total_timesteps=20000)\n",
    "\n",
    "    # 提取中间层（隐藏层）权重\n",
    "    encoder_state = {\n",
    "        k: v for k, v in model.policy.mlp_extractor.state_dict().items()\n",
    "        if \"1\" in k or \"2\" in k  # 只保留第二层及以后的参数（跳过输入层）\n",
    "    }\n",
    "    torch.save(encoder_state, \"small_encoder_hidden.pt\")\n",
    "    model.save(\"ppo_small_model\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_large_model_with_transfer():\n",
    "    env = DronePlacementEnv()\n",
    "    \n",
    "    policy_kwargs = dict(net_arch=[dict(pi=[64, 64], vf=[64, 64])])\n",
    "    model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "\n",
    "    # 加载隐藏层参数（跳过输入层）\n",
    "    encoder_weights = torch.load(\"small_encoder_hidden.pt\")\n",
    "    model.policy.mlp_extractor.load_state_dict(encoder_weights, strict=False)\n",
    "\n",
    "    model.learn(total_timesteps=100000)\n",
    "    model.save(\"ppo_large_model\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_small_model_encoder()           # 先训练并保存小地图 encoder\n",
    "    train_large_model_with_transfer()    # 迁移 encoder 到大地图继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef0794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Placed 5x5 drone at (14, 4), reward: 24.60\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0  0  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 1: Placed 5x5 drone at (17, 1), reward: 18.60\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 2: Placed 3x3 drone at (9, 5), reward: 8.80\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 3: Placed 5x5 drone at (17, 7), reward: 18.60\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0 -2  0 -1 -1 -1  0]\n",
      " [-2 -2 -2 -2  0  0  0  0  0 -2  0 -1  0  0  0]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 4: Placed 5x5 drone at (6, 8), reward: 22.60\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0 -2  0 -1 -1 -1  0]\n",
      " [-2 -2 -2 -2  0  0  0  0  0 -2  0 -1  0  0  0]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 5: Placed 5x5 drone at (17, 1), reward: -0.40\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0 -2  0 -1 -1 -1  0]\n",
      " [-2 -2 -2 -2  0  0  0  0  0 -2  0 -1  0  0  0]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 6: Placed 3x3 drone at (11, 1), reward: 1.80\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0 -2  0 -1 -1 -1  0]\n",
      " [-2 -2 -2 -2  0  0  0  0  0 -2  0 -1  0  0  0]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 7: Placed 3x3 drone at (7, 6), reward: 9.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0 -2  0 -1 -1 -1  0]\n",
      " [-2 -2 -2 -2  0  0  0  0  0 -2  0 -1  0  0  0]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n",
      "Step 8: Placed 5x5 drone at (21, 8), reward: 6.90\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [-2 -2 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [-2 -2 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 9: Placed 3x3 drone at (19, 5), reward: 6.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 10: Placed 3x3 drone at (3, 2), reward: 8.80\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -1 -1  0 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 11: Placed 5x5 drone at (17, 4), reward: -0.40\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -1 -1  0 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 12: Placed 3x3 drone at (21, 7), reward: 0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -1 -1  0 -2 -2 -1 -1  0  0 -1 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1 -2 -2 -2 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 13: Placed 5x5 drone at (7, 1), reward: 14.60\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0 -2]\n",
      " [-2  0  0  0 -2 -2  0  0  0  0  0  0  0  0 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 14: Placed 3x3 drone at (3, 2), reward: -0.20\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0 -2]\n",
      " [-2  0  0  0 -2 -2  0  0  0  0  0  0  0  0 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 15: Placed 3x3 drone at (4, 5), reward: 2.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0 -2]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0 -2 -2 -2]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 16: Placed 3x3 drone at (11, 8), reward: 7.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -2 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 17: Placed 3x3 drone at (14, 0), reward: 0.80\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0 -2]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0 -2]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0 -2]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 18: Placed 5x5 drone at (7, 2), reward: 3.60\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [-2 -2 -2 -2 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 19: Placed 5x5 drone at (12, 0), reward: 11.90\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0 -2 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 20: Placed 3x3 drone at (18, 6), reward: -0.20\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0 -2 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 21: Placed 5x5 drone at (13, 4), reward: -0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0 -2 -2 -2  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1 -2  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1 -2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0 -2 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 22: Placed 5x5 drone at (9, 3), reward: 6.60\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 23: Placed 3x3 drone at (4, 5), reward: 0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 24: Placed 5x5 drone at (10, 4), reward: -0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 25: Placed 3x3 drone at (2, 6), reward: 6.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 26: Placed 5x5 drone at (8, 9), reward: -0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 27: Placed 5x5 drone at (13, 2), reward: -0.40\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1 -2 -2 -2 -2]]\n",
      "Step 28: Placed 5x5 drone at (22, 2), reward: 3.90\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 29: Placed 5x5 drone at (9, 2), reward: -0.40\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 30: Placed 3x3 drone at (5, 6), reward: -0.20\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 31: Placed 3x3 drone at (21, 9), reward: 0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 32: Placed 5x5 drone at (7, 5), reward: -0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0 -2 -2  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0 -2 -2 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 33: Placed 3x3 drone at (21, 2), reward: 4.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [-2 -2 -2 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0 -2 -2 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 34: Placed 5x5 drone at (19, 6), reward: 5.90\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 35: Placed 3x3 drone at (13, 4), reward: 0.10\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 36: Placed 3x3 drone at (9, 5), reward: -0.20\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 37: Placed 3x3 drone at (14, 2), reward: -0.20\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 38: Placed 3x3 drone at (12, 5), reward: -0.20\n",
      "[[-2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [-2  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 39: Placed 3x3 drone at (1, 6), reward: 4.10\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0 -2  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0 -2  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 40: Placed 3x3 drone at (15, 2), reward: 1.80\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 41: Placed 5x5 drone at (13, 7), reward: -0.40\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 42: Placed 5x5 drone at (19, 2), reward: -0.40\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 43: Placed 5x5 drone at (20, 7), reward: -0.10\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0 -2 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1 -2 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 44: Placed 3x3 drone at (1, 2), reward: 2.10\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1  0 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 45: Placed 5x5 drone at (15, 5), reward: -0.40\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1  0 -2]\n",
      " [ 0 -1 -1  0 -2 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 46: Placed 3x3 drone at (4, 8), reward: 1.80\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1  0 -2]\n",
      " [ 0 -1 -1  0  0 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 47: Placed 3x3 drone at (6, 0), reward: 0.10\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1  0 -2]\n",
      " [ 0 -1 -1  0  0 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 48: Placed 5x5 drone at (12, 4), reward: -0.40\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1  0 -2]\n",
      " [ 0 -1 -1  0  0 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "Step 49: Placed 5x5 drone at (17, 3), reward: -5.40\n",
      "[[ 0  0  0 -2 -2 -2 -2 -2 -2 -2  0  0  0  0 -2]\n",
      " [ 0  0  0  0 -2 -2 -2 -2 -2 -2  0  0 -1  0 -2]\n",
      " [ 0 -1 -1  0  0 -2 -1 -1  0  0 -1  0  0  0  0]\n",
      " [ 0  0  0  0  0 -2  0  0  0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0]\n",
      " [ 0  0 -1  0  0  0 -1  0  0  0 -1  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 -1  0  0 -1  0  0  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0]\n",
      " [-1  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0 -1 -1  0  0  0 -1 -1 -1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  0 -2 -2  0  0  0  0 -1  0  0  0  0]]\n",
      "🎉 Mission Complete: All cells explored!\n"
     ]
    }
   ],
   "source": [
    "def run_trained_model():\n",
    "    model = PPO.load(\"ppo_large_model\")\n",
    "    env = DronePlacementEnv()\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    for step in range(env.max_steps):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        x = action // 20\n",
    "        y = (action % 20) // 2\n",
    "        drone_type = \"3x3\" if action % 2 == 0 else \"5x5\"\n",
    "        print(f\"Step {step}: Placed {drone_type} drone at ({x}, {y}), reward: {reward:.2f}\")\n",
    "        env.render()\n",
    "        if terminated or truncated:\n",
    "            print(\"🎉 Mission Complete: All cells explored!\")\n",
    "            break\n",
    "        if truncated:\n",
    "            print(\"⚠️ Max steps reached.\")\n",
    "            break\n",
    "\n",
    "# === Entry Point ===\n",
    "if __name__ == \"__main__\":\n",
    "    run_trained_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchsafe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
